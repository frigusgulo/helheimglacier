{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy as lp\n",
    "import numpy as np\n",
    "import scipy\n",
    "from laspy.file import File\n",
    "from scipy.spatial.kdtree import KDTree\n",
    "import torch \n",
    "import pickle\n",
    "import time as time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from numpy.random import (normal,uniform)\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "from matplotlib.patches import Circle\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import itertools\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cluster():\n",
    "    def __init__(self, points):\n",
    "        self.points = np.array(points)\n",
    "        self.roughness = np.nanstd(self.points[:,-1])\n",
    "\n",
    "    def boundpoints(self):\n",
    "        x_min = np.min(self.points[:,0])\n",
    "        x_max = np.max(self.points[:,0])\n",
    "        y_min = np.min(self.points[:,1])\n",
    "        y_max = np.max(self.points[:,1])\n",
    "        return np.array([[x_min,x_max],[y_min,y_max]])\n",
    "\n",
    "\n",
    "class scan(cluster):\n",
    "    def __init__(self,filepath):\n",
    "        start = time.time()\n",
    "        self.file = File(filepath,mode=\"r\")\n",
    "        self.scale = self.file.header.scale[0]\n",
    "        self.offset = self.file.header.offset[0]\n",
    "        self.tree = KDTree(np.vstack([self.file.x, self.file.y, self.file.z]).transpose())\n",
    "        self.time = self.file.header.get_date()\n",
    "        end = time.time() - start\n",
    "        print(\"Time Elapsed: {}\".format(end))\n",
    "\n",
    "    def nearNeighbor(self,point,k=1):\n",
    "        return self.tree.query(point,k=k)\n",
    "\n",
    "    def radialcluster(self,point,radius):\n",
    "        neighbor = self.tree.data[self.tree.query(point,k=1)[1]]\n",
    "        points = self.tree.data[self.tree.query_ball_point(neighbor,radius)]\n",
    "        return np.array(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "1) Generate a set of clusters in an observer class with a time stamp\n",
    "\n",
    "2) Generate hypothesis points according to a normal distribution\n",
    "\n",
    "3) Weight each hypothesis point with F(Hypthesis, Cluster) function\n",
    "\n",
    "4) Resample \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particleset():  \n",
    "    \n",
    "    def __init__(self,points=None,center=None,weight=None):\n",
    "        self.points = points\n",
    "        self.center = center\n",
    "        self.weight = weight\n",
    "        \n",
    "    def set_weight(self,weight):\n",
    "        self.weight = weight\n",
    "        \n",
    "\n",
    "class Observer(Particleset):\n",
    "    def __init__(self,obs):\n",
    "        self.observations = obs\n",
    "        self.observations.sort(key= lambda scan: scan.time)\n",
    "        self.particles = None\n",
    "        self.test = Particleset()\n",
    "        \n",
    "    def particle_centroid(self,mean, std, N):\n",
    "        particles = np.empty((N, 3))\n",
    "        particles[:, 0] = normal(mean[0],std[0],N) # x pos\n",
    "        particles[:, 1] = normal(mean[1],std[1],N) # y pos\n",
    "        particles[:, 2] = normal(mean[2],std[2],N)\n",
    "        return particles\n",
    "    \n",
    "    def particle_clusters(self,particles,observer_index):\n",
    "        particle_set = []\n",
    "        for i in range(particles.shape[0]):\n",
    "            points = self.observations[observer_index].radialcluster(particles[i,:],10)\n",
    "            cluster = Particleset(points=points,center=particles[i,:])\n",
    "            particle_set.append(cluster)\n",
    "        return particle_set\n",
    "    \n",
    "    def gen_particles(self,mean,std,N,obs_indx):\n",
    "        particles = self.particle_centroid(mean,std,N)\n",
    "        self.particles = self.particle_clusters(particles,obs_indx)\n",
    "        return self.particles\n",
    "        \n",
    "    ###################\n",
    "    def kernel(self,x):\n",
    "        return ((1.0/(2.0*np.pi))**3.0)*np.exp(-(x/2.0))\n",
    "        \n",
    "    def d_sigma(self,X,Y):\n",
    "        # X is hypothesis, list of particle sets\n",
    "        # Y is the reference cluster\n",
    "\n",
    "        ref = [Y[i,:] for i in range(Y.shape[0])]\n",
    "        hyp = [X[i,:] for i in range(X.shape[0])]\n",
    "        mat = []\n",
    "        '''\n",
    "        #if isinstance(X,type(self.test)) :\n",
    "        for cluster in X:\n",
    "            points = [cluster.points[i,:] for i in range(cluster.points.shape[0])]\n",
    "            dists = np.array([np.abs(xy[0] - xy[1]) for xy in itertools.product(points,ref)])\n",
    "            mat.append(np.mean(dists,axis=0))\n",
    "        #else:\n",
    "           # mat = X - Y\n",
    "        '''\n",
    "        mat = np.array([np.abs(xy[0] - xy[1]) for xy in itertools.product(hyp,ref)])\n",
    "        #mat = np.array(mat)\n",
    "        mat = mat.T@mat\n",
    "        try:\n",
    "            L = np.linalg.cholesky(mat)\n",
    "            H = np.linalg.inv(L)\n",
    "            H = H.T@H\n",
    "            return np.sqrt(mat.T@H@mat)\n",
    "        except:\n",
    "            print(\"Non - Singular \\n\")\n",
    "            return 0\n",
    "    \n",
    "    def bandwidth(self,X):\n",
    "        return np.min(self.d_sigma(X,X))\n",
    "    \n",
    "    def reach_distance(self,X,Y):\n",
    "        A = self.d_sigma(X,Y)\n",
    "        B = self.d_sigma(Y,Y)\n",
    "        if np.sum(A) > np.sum(B):\n",
    "            return A\n",
    "        else:\n",
    "            return B\n",
    "        \n",
    "        \n",
    "    def evaluate(self,X,Y):\n",
    "        # X is hypothesis\n",
    "        # Y is reference\n",
    "        A = self.reach_distance(X,Y)\n",
    "        B = self.bandwidth(Y)\n",
    "        C = (1/(B**3))*self.kernel(A/B)\n",
    "        return(np.mean(C))\n",
    "       \n",
    "    def weight_cluster(self,X,Y):\n",
    "        # assign a cluster weight given a reference cluster\n",
    "        # X is hypothesis\n",
    "        # Y is reference\n",
    "        for particleset in X:\n",
    "            weights = self.evaluate(particleset.points,Y)\n",
    "            particleset.set_weight(weights)\n",
    "       \n",
    "    \n",
    "    def resample(self,particle_set):\n",
    "        weights = np.array([points.weight for points in particle_set])\n",
    "        cumsum = np.cumsum(weights)\n",
    "        indexes = np.searchsorted(cumsum,uniform(0,1,np.max(weights.shape)))\n",
    "        for i, particle in enumerate(particle_set):\n",
    "            particle.weights = particle_set[indexes[i]].weights\n",
    "            \n",
    "    ####################################\n",
    "    def update(self,particle_set,vels):\n",
    "        for i, _ in enumerate(particle_set):\n",
    "            for j,_ in enumerate(particles.points):\n",
    "                particle_set[i].points[j,:] += vels\n",
    "            \n",
    "        \n",
    "    def estimate(self,particle_set):\n",
    "        positions = [cluster.center for cluster in particles]\n",
    "        weights = [particle.weight for particle in particle_set]\n",
    "        mean = np.average(positions,weights=weights)\n",
    "        var = np.var(positions,weights=weights)\n",
    "        return mean, np.linalg.norm(var)  #returns the estimated location\n",
    "    \n",
    "    def predict(self,point,guess,ref_index,test_index):\n",
    "        ref_cluster = self.observations[ref_index].radialcluster(point,10)\n",
    "        hypothesis = self.gen_particles(guess,np.array([10,10,10]),1000,test_index)\n",
    "        self.weight_cluster(hypothesis,ref_cluster)\n",
    "        self.resample(hypothesis)\n",
    "        return self.estimate(hypothesis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 124.8684458732605\n",
      "Time Elapsed: 117.87526035308838\n"
     ]
    }
   ],
   "source": [
    "files = glob(join(\"/home/dunbar/Research/helheim/data/misc/lazfiles\",\"*.laz\"))\n",
    "scans = [scan(file) for file in files[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess(point):\n",
    "    dt = np.array([10,5,1])\n",
    "    point += dt\n",
    "    return point\n",
    "start = np.array([534330.24,7361293.11,175])\n",
    "guess = guess(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c837b524cc38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObserver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-3ba47407db16>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, point, guess, ref_index, test_index)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_particles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3ba47407db16>\u001b[0m in \u001b[0;36mresample\u001b[0;34m(self, particle_set)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticle_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mparticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparticle_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "observer = Observer(scans)\n",
    "estimate = observer.predict(point=start,guess=guess,ref_index=0,test_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
