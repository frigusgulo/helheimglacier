{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform classification tests on reference vs hypothesis clusters\n",
    "\n",
    "import numpy as np\n",
    "import laspy as lp\n",
    "import numpy as np\n",
    "import scipy\n",
    "from laspy.file import File\n",
    "from scipy.spatial.kdtree import KDTree\n",
    "import time as time\n",
    "import scipy.stats\n",
    "from numpy.random import (normal,uniform)\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "from matplotlib.patches import Circle\n",
    "from glob import glob\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doug,\n",
    "I have come up with something of an experiment that evaluates the accuracy of this approach to \"labeling\" hypothesis clusters.\n",
    "This was inspired predominately by what I have read in https://iis.uibk.ac.at/public/papers/Xiong-2013-3DV.pdf (Section 3.1-3.3)\n",
    "\n",
    " I formulated my approach as follows:\n",
    "\n",
    "Given a cluster of reference points\n",
    "and a set of test clusters:\n",
    "\n",
    "1) On a grid of \"centers\" in the UTM coordinate system, retrieve from the KD tree  a cluster of 100 points that are Nearest Neighbors to the center \n",
    "and assign a respective label to each cluster [0-99]\n",
    "\n",
    "2) Create 10 \"test\" clusters from each labeled cluster: Perturb these points with white gaussian noise and assign their true label to that of the labeled cluster\n",
    "\n",
    "3) Iterate through all the perturbed \"test\" clusters and compute the expected likelihood between them and each labeled cluster, assigning the lilihood of each labeled\n",
    "cluster to the \"testlabel\" array\n",
    "\n",
    "4) Compute accuracy by the number of times the largest computed likelihood corresponds to the true label of the cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scan():\n",
    "    def __init__(self,filepath):\n",
    "        start = time.time()\n",
    "        self.file = File(filepath,mode=\"r\")\n",
    "        self.scale = self.file.header.scale[0]\n",
    "        self.offset = self.file.header.offset[0]\n",
    "        self.tree = KDTree(np.vstack([self.file.x, self.file.y, self.file.z]).transpose())\n",
    "        self.time = self.file.header.get_date()\n",
    "        end = time.time() - start\n",
    "        print(\"Time Elapsed: {}\".format(end))\n",
    "\n",
    "    def nearNeighbor(self,point,k=1):\n",
    "        return self.tree.query(point,k=k)\n",
    "    \n",
    "    def NNN(self,point,k):\n",
    "        return self.tree.data[self.tree.query(point,k=k)[1]]\n",
    "    \n",
    "    def radialcluster(self,point,radius):\n",
    "        neighbor = self.tree.data[self.tree.query(point,k=1)[1]]\n",
    "        points = self.tree.data[self.tree.query_ball_point(neighbor,radius)]\n",
    "        return np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particleset():  \n",
    "    \n",
    "    def __init__(self,points=None,center=None,label=None,weight=None):\n",
    "        self.points = points\n",
    "        self.center = center\n",
    "        self.weight = weight\n",
    "        self.gen_covariance()\n",
    "        self.testlabel = np.zeros((1,100))\n",
    "        self.label = label\n",
    "    def set_weight(self,weight):\n",
    "        self.weight = weight\n",
    "\n",
    "    def settestlabel(self,weight,index):\n",
    "        self.testlabel[:,index] = weight\n",
    "\n",
    "    def gen_covariance(self):\n",
    "        if self.points is not None:\n",
    "            self.cov = np.cov(m=self.points.T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer(Particleset):\n",
    "    def __init__(self,obs):\n",
    "        self.observations = obs\n",
    "        self.observations.sort(key= lambda scan: scan.time)\n",
    "        #self.particles = None\n",
    "\n",
    "        \n",
    "    def kernel_function(self,test,ref):\n",
    "        # Evaluate the liklihood of one KDE under another\n",
    "        mean_test = test.points\n",
    "        cov_test = test.cov\n",
    "        mean_ref = ref.points\n",
    "        cov_ref = ref.cov\n",
    "        assert mean_test.shape == mean_ref.shape\n",
    "        gamma = mean_test - mean_ref\n",
    "        sigma = cov_test + cov_ref\n",
    "        A = 1/((2*np.pi**(3/2))*(np.linalg.det(sigma)**(.5)))\n",
    "        B = np.exp((-.5)*gamma@np.linalg.inv(sigma)@gamma.T)\n",
    "        C = 1/(np.max(mean_test.shape))\n",
    "        return (C**2)*np.sum(A*B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(join(\"/home/dunbar/Research/helheim/data/misc/lazfiles\",\"*.laz\"))\n",
    "refscan = scan(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "easting = np.linspace(532653.83,534937.46,10)\n",
    "northing = np.linspace(7361772.42,7361124.13,10)\n",
    "grid = [np.array([x,y,1000]) for x in easting for y in northing]\n",
    "labeled_scans = [Particleset(points=refscan.NNN(point,k=100),label=i) for i,point in enumerate(grid)] # Step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_points(points):\n",
    "    n,m = points.shape\n",
    "    new = points.copy()\n",
    "    for i in range(m):\n",
    "        noise = np.random.normal(0,3,n)\n",
    "        new[:,i] += noise.T\n",
    "\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "testscans = []\n",
    "for scan in labeled_scans:\n",
    "    points = scan.points\n",
    "    for i in range(10):\n",
    "        set_ = perturb_points(points)\n",
    "        testscans.append(Particleset(points=set_,label=scan.label)) # Step 2\n",
    "\n",
    "    observer = Observer([refscan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "for testscan in testscans:\n",
    "    for labelscan in labeled_scans:\n",
    "        weight = observer.kernel_function(testscan,labelscan) # Step 3\n",
    "        testscan.settestlabel(weight,labelscan.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ste\n",
    "count = 0\n",
    "for testscan in testscans:\n",
    "    if int(np.argmax(testscan.testlabel)) == int(testscan.label): # Step 4\n",
    "        count += 1\n",
    "print(\"Classification Accuracy: {}\".format(count/1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
