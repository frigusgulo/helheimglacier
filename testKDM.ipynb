{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy as lp\n",
    "import numpy as np\n",
    "import scipy\n",
    "from laspy.file import File\n",
    "from scipy.spatial.kdtree import KDTree\n",
    "import torch \n",
    "import pickle\n",
    "import time as time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from numpy.random import (normal,uniform)\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "from matplotlib.patches import Circle\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import itertools\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cluster():\n",
    "    def __init__(self, points):\n",
    "        self.points = np.array(points)\n",
    "        self.roughness = np.nanstd(self.points[:,-1])\n",
    "\n",
    "    def boundpoints(self):\n",
    "        x_min = np.min(self.points[:,0])\n",
    "        x_max = np.max(self.points[:,0])\n",
    "        y_min = np.min(self.points[:,1])\n",
    "        y_max = np.max(self.points[:,1])\n",
    "        return np.array([[x_min,x_max],[y_min,y_max]])\n",
    "\n",
    "\n",
    "class scan(cluster):\n",
    "    def __init__(self,filepath):\n",
    "        start = time.time()\n",
    "        self.file = File(filepath,mode=\"r\")\n",
    "        self.scale = self.file.header.scale[0]\n",
    "        self.offset = self.file.header.offset[0]\n",
    "        self.tree = KDTree(np.vstack([self.file.x, self.file.y, self.file.z]).transpose())\n",
    "        self.time = self.file.header.get_date()\n",
    "        end = time.time() - start\n",
    "        print(\"Time Elapsed: {}\".format(end))\n",
    "\n",
    "    def nearNeighbor(self,point,k=1):\n",
    "        return self.tree.query(point,k=k)\n",
    "    \n",
    "    def NNN(self,point,k):\n",
    "        return self.tree.data[self.tree.query(point,k=k)[1]]\n",
    "    \n",
    "    def radialcluster(self,point,radius):\n",
    "        neighbor = self.tree.data[self.tree.query(point,k=1)[1]]\n",
    "        points = self.tree.data[self.tree.query_ball_point(neighbor,radius)]\n",
    "        return np.array(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "1) Generate a set of clusters in an observer class with a time stamp\n",
    "\n",
    "2) Generate hypothesis points according to a normal distribution\n",
    "\n",
    "3) Weight each hypothesis point with F(Hypthesis, Cluster) function\n",
    "\n",
    "4) Resample \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particleset():  \n",
    "    \n",
    "    def __init__(self,points=None,center=None,weight=None):\n",
    "        self.points = points\n",
    "        self.center = center\n",
    "        self.weight = weight\n",
    "        \n",
    "    def set_weight(self,weight):\n",
    "        self.weight = weight\n",
    "        \n",
    "    def gen_covariance(self):\n",
    "        if self.points is not None:\n",
    "            self.cov = np.cov(m=self.points.T)\n",
    "        \n",
    "\n",
    "class Observer(Particleset):\n",
    "    def __init__(self,obs):\n",
    "        self.observations = obs\n",
    "        self.observations.sort(key= lambda scan: scan.time)\n",
    "        #self.particles = None\n",
    "        self.test = Particleset()\n",
    "        \n",
    "    def particle_centroid(self,mean, std, N):\n",
    "        particles = np.empty((N, 3))\n",
    "        particles[:, 0] = normal(mean[0],std[0],N) # x pos\n",
    "        particles[:, 1] = normal(mean[1],std[1],N) # y pos\n",
    "        particles[:, 2] = normal(mean[2],std[2],N)\n",
    "        return particles\n",
    "    \n",
    "    def particle_clusters(self,particles,observer_index):\n",
    "        particle_set = []\n",
    "        for i in range(particles.shape[0]):\n",
    "            points = self.observations[observer_index].NNN(particles[i,:],k=100)\n",
    "            cluster = Particleset(points=points,center=particles[i,:])\n",
    "            cluster.gen_covariance()\n",
    "            particle_set.append(cluster)\n",
    "        return particle_set\n",
    "    \n",
    "    def gen_particles(self,mean,std,N,obs_indx):\n",
    "        particles = self.particle_centroid(mean,std,N)\n",
    "        particle_set = self.particle_clusters(particles,obs_indx)\n",
    "        return particle_set\n",
    "        \n",
    "    def kernel_function(self,test,ref):\n",
    "        # Evaluate the liklihood of one KDE under another\n",
    "        mean_test = test.points\n",
    "        cov_test = test.cov\n",
    "        mean_ref = ref.points\n",
    "        cov_ref = ref.cov\n",
    "        assert mean_test.shape == mean_ref.shape\n",
    "        gamma = mean_test - mean_ref\n",
    "        sigma = cov_test + cov_ref\n",
    "        A = 1/((2*np.pi**(3/2))*(np.linalg.det(sigma)**(.5)))\n",
    "        B = np.exp((-.5)*gamma@np.linalg.inv(sigma)@gamma.T)\n",
    "        C = 1/(np.max(mean_test.shape))\n",
    "        return np.sum(A*B)\n",
    "    \n",
    "    def evaluate(self,testset,ref):\n",
    "        for cluster in testset:\n",
    "            weight = self.kernel_function(cluster,ref)\n",
    "            cluster.set_weight(weight)\n",
    "            \n",
    "        \n",
    "    ###################\n",
    "    '''\n",
    "    def kernel(self,x):\n",
    "        return ((1.0/(2.0*np.pi))**3.0)*np.exp(-(np.linalg.norm(x)**2/2.0))\n",
    "        \n",
    "    def d_sigma(self,X,Y):\n",
    "        # X is hypothesis, list of particle sets\n",
    "        # Y is the reference cluster\n",
    "\n",
    "        ref = [Y[i,:] for i in range(Y.shape[0])]\n",
    "        hyp = [X[i,:] for i in range(X.shape[0])]\n",
    "        mat = []\n",
    "        \n",
    "        #if isinstance(X,type(self.test)) :\n",
    "        for cluster in X:\n",
    "            points = [cluster.points[i,:] for i in range(cluster.points.shape[0])]\n",
    "            dists = np.array([np.abs(xy[0] - xy[1]) for xy in itertools.product(points,ref)])\n",
    "            mat.append(np.mean(dists,axis=0))\n",
    "        #else:\n",
    "           # mat = X - Y\n",
    "        \n",
    "        mat = np.array([np.abs(xy[0] - xy[1]) for xy in itertools.product(hyp,ref)])\n",
    "        #mat = np.array(mat)\n",
    "        mat = mat.T@mat\n",
    "        try:\n",
    "            L = np.linalg.cholesky(mat)\n",
    "            H = np.linalg.inv(L)\n",
    "            H = H.T@H\n",
    "            return np.sqrt(mat.T@H@mat)\n",
    "        except:\n",
    "            print(\"Non - Singular \\n\")\n",
    "            return 0\n",
    "    \n",
    "    def bandwidth(self,X):\n",
    "        return np.min(self.d_sigma(X,X))\n",
    "    \n",
    "    def reach_distance(self,X,Y):\n",
    "        A = self.d_sigma(X,Y)\n",
    "        B = self.d_sigma(Y,Y)\n",
    "        if np.sum(A) > np.sum(B):\n",
    "            return A\n",
    "        else:\n",
    "            return B\n",
    "        \n",
    "        \n",
    "    def evaluate(self,X,Y):\n",
    "        # X is hypothesis\n",
    "        # Y is reference\n",
    "        A = self.reach_distance(X,Y)\n",
    "        B = self.bandwidth(Y)\n",
    "        C = (1/(B**3))*self.kernel(A/B)\n",
    "        return(np.mean(C))\n",
    "    \n",
    "    def weight_cluster(self,X,Y):\n",
    "        # assign a cluster weight given a reference cluster\n",
    "        # X is hypothesis\n",
    "        # Y is reference\n",
    "        for particleset in X:\n",
    "            weights = self.evaluate(particleset.points,Y)\n",
    "            particleset.set_weight(weights)\n",
    "       \n",
    "    '''\n",
    "    def resample(self,particle_set):\n",
    "        weights = np.array([points.weight for points in particle_set])\n",
    "        cumsum = np.cumsum(weights)\n",
    "        B = uniform(0,1,np.max(weights.shape))\n",
    "        indexes = np.squeeze(np.searchsorted(cumsum,B))\n",
    "        print(indexes)\n",
    "        print(weights.shape)\n",
    "        print(cumsum)\n",
    "        print(B)\n",
    "        for i, particle in enumerate(particle_set):\n",
    "                particle.weight = particle_set[indexes[i]].weight\n",
    "\n",
    "        \n",
    "    def estimate(self,particle_set):\n",
    "        positions = [cluster.center for cluster in particle_set]\n",
    "        weights = [particle.weight for particle in particle_set]\n",
    "        mean = np.average(positions,axis=0,weights=weights)\n",
    "        var = np.var(positions,axis=0)\n",
    "        return mean, np.linalg.norm(var)  #returns the estimated location\n",
    "    \n",
    "    def predict(self,point,guess,ref_index,test_index):\n",
    "        ref_cluster = Particleset(points=self.observations[ref_index].NNN(point,k=100))\n",
    "        ref_cluster.gen_covariance()\n",
    "        hypothesis = self.gen_particles(guess,np.array([5,5,1]),50,test_index)\n",
    "        self.evaluate(hypothesis,ref_cluster)\n",
    "        self.resample(hypothesis)\n",
    "        return self.estimate(hypothesis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(join(\"/home/dunbar/Research/helheim/data/misc/lazfiles\",\"*.laz\"))\n",
    "scans = [scan(file) for file in files[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observer = Observer(scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess(point):\n",
    "    dt = np.array([10,10,5])\n",
    "    point += dt\n",
    "    return point\n",
    "start = np.array([534330.24,7361293.11,175])\n",
    "guess = guess(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate = observer.predict(point=start,guess=guess,ref_index=0,test_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.10748976, 0.88610618, 0.07415133])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displacement = estimate[0] - start\n",
    "np.abs(displacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
